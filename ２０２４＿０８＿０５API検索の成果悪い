import requests
import os
import pandas as pd
from xml.etree import ElementTree as ET

def fetch_pubmed_data_get(journal=None, volume=None, page=None, authors=None):
    base_url = "https://pubmed.ncbi.nlm.nih.gov/api/citmatch/"
    method = "field"

    # Prepare query parameters with URL encoding for fuzzy search
    params = {
        "method": method,
        "journal": f'"{journal}"' if journal else None,
        "volume": volume,
        "page": page,
        "authors": f'"{authors}"' if authors else None
    }

    # Filter out empty parameters and construct the query string
    query_string = "&".join([f"{key}={requests.utils.quote(str(value))}" for key, value in params.items() if value])
    url = f"{base_url}?{query_string}"

    print(f"Generated link: {url}")  # Debug output

    response = requests.get(url)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Error: {response.status_code}")  # Debug output
        print(response.text)  # Debug output
        return {"error": f"Failed to fetch data, status code: {response.status_code}", "details": response.text}

def fetch_pubmed_data_post(journal=None, volume=None, page=None, authors=None):
    base_url = "https://pubmed.ncbi.nlm.nih.gov/api/citmatch/"
    headers = {'Content-Type': 'application/json'}
    
    data = {
        "citmatch": {
            "method": "field",
            "journal": journal,
            "volume": volume,
            "page": page,
            "authors": [
                {
                    "name": authors,
                    "position": "first"
                }
            ]
        }
    }

    response = requests.post(base_url, headers=headers, json=data)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Error: {response.status_code}")  # Debug output
        print(response.text)  # Debug output
        return {"error": f"Failed to fetch data, status code: {response.status_code}", "details": response.text}

def fetch_article_details(uid):
    """Fetch article details from PubMed given a UID."""
    base_url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    db = "pubmed"
    retmode = "xml"

    params = {
        "db": db,
        "retmode": retmode,
        "id": uid
    }

    response = requests.get(base_url, params=params)
    if response.status_code == 200:
        return response.text  # For simplicity, return the XML content
    else:
        print(f"Error fetching article {uid}: {response.status_code}")
        return None

def parse_article_details(xml_data, uid):
    """Parse the XML data to extract relevant article details."""
    root = ET.fromstring(xml_data)
    title = root.findtext(".//ArticleTitle")
    link = f"https://pubmed.ncbi.nlm.nih.gov/{uid}/"
    return title, link

def main():
    print("PubMed Search")
    journal = input("Enter journal name (or leave blank): ")
    volume = input("Enter volume (or leave blank): ")
    page = input("Enter page number (or leave blank): ")
    authors = input("Enter authors (format: Surname Initial, or leave blank): ")

    # Ensure at least one parameter is provided
    if not (journal or volume or page or authors):
        print("Error: At least one search parameter must be provided.")
        return

    # Perform GET request
    result_get = fetch_pubmed_data_get(journal, volume, page, authors)
    
    if "error" in result_get:
        print("GET request error:", result_get)
    else:
        print("GET request response:", result_get)

    # Perform POST request
    result_post = fetch_pubmed_data_post(journal, volume, page, authors)
    
    if "error" in result_post:
        print("POST request error:", result_post)
    else:
        print("POST request response:", result_post)
    
    titles = []
    links = []
    if "result" in result_post and "uids" in result_post["result"]:
        for entry in result_post["result"]["uids"]:
            uid = entry["pubmed"]
            details = fetch_article_details(uid)
            if details:
                title, link = parse_article_details(details, uid)
                titles.append(title)
                links.append(link)
    
    if not titles:
        print("No results found.")
        return
    
    # 解析したデータを辞書に保存
    data = {
        'Title': titles,
        'Link': links
    }

    # データフレームを作成
    df = pd.DataFrame(data)

    # 出力ファイルパスをユーザーのドキュメントフォルダに設定
    output_directory = os.path.join(os.path.expanduser('~'), 'Documents')
    output_file_path = os.path.join(output_directory, 'ncbi_result_info.xlsx')

    # データフレームをExcelファイルに書き出す
    df.to_excel(output_file_path, index=False, engine='openpyxl')

    print(f"Excelファイル '{output_file_path}' に保存されたよ！")

if __name__ == "__main__":
    main()
